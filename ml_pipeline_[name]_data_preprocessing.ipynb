{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37BV7-CMEbMj",
   "metadata": {
    "id": "37BV7-CMEbMj"
   },
   "source": [
    "# Data Processing Approach for Portfolio Project\n",
    "\n",
    "## Project Title: Dropout Predictor\n",
    "\n",
    "![Logo](images/logo-grayscale.png)\n",
    "\n",
    "## Student Name: Elvis Guy Bakunzi\n",
    "\n",
    "---\n",
    "\n",
    "1. **Data Sources and Aggregation:**\n",
    "   - List all sources of data for the project. **You must consider sources outside kaggle, google datasets** (insert links where necessary to online platforms,research papers etc)\n",
    "\n",
    "   Sources:\n",
    "      - [Mendeley Data](https://data.mendeley.com/datasets/5b82ytz489/1) : Mendeley Data is a free and secure cloud-based research data repository that allows researchers to store, share, and cite their data easily.\n",
    "      - [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) : The UCI Machine Learning Repository is a collection of over 550 datasets, databases, domain theories, and data generators that are widely used for machine learning research and experimentation.\n",
    "      - [Kaggle](https://www.kaggle.com/datasets/naveenkumar20bps1137/predict-students-dropout-and-academic-success?select=dataset.csv) : Kaggle is the world's largest data science community and a platform for data science competitions, offering tools and resources for data scientists and machine learning practitioners to collaborate, learn, and achieve their goals.\n",
    "   \n",
    "   \n",
    "**Determine if data aggregation from multiple sources is necessary for comprehensive analysis.***\n",
    "\n",
    "  Yes, data aggregation from multiple sources is often necessary for comprehensive analysis.  This is because:\n",
    "\n",
    "  - It increases Broader Insights, by aggregating data from multiple sources, you gain a wider range of perspectives.\n",
    "  - It provides Data Completeness, No single source is likely to encompass all relevant information. Pulling data from multiple sources fills in gaps, reduces biases, and creates a more complete and reliable dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K6W_BkY6FGRL",
   "metadata": {
    "id": "K6W_BkY6FGRL"
   },
   "outputs": [],
   "source": [
    "#insert code if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TzyeghJDEjit",
   "metadata": {
    "id": "TzyeghJDEjit"
   },
   "source": [
    "\n",
    "\n",
    "2. **Data Format Transformation:**\n",
    "   - Describe the current format of the data.\n",
    "   - Outline the planned transformation to a unified format suitable for analysis and modeling.\n",
    "\n",
    " **Your answer for data transformation goes here **\n",
    "\n",
    "3. **Data Exploration:**\n",
    "   - Enumerate the features included in the dataset.\n",
    "   \n",
    "   - Summarize findings from exploratory data analysis (EDA) including distributions, correlations, and outliers.\n",
    "   \n",
    "  **Insert code for data exploration below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oYEgIOxSFUHW",
   "metadata": {
    "id": "oYEgIOxSFUHW"
   },
   "outputs": [],
   "source": [
    "#Include plots for EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHKtxjFKFNpB",
   "metadata": {
    "id": "tHKtxjFKFNpB"
   },
   "source": [
    "\n",
    "\n",
    "4. **Hypothesis Testing:**\n",
    "   - State any preexisting hypotheses about the data.\n",
    "   - Explain methodologies to empirically test these hypotheses.\n",
    "\n",
    "   **Your answer for Hypothesis Testing goes here **\n",
    "\n",
    "5. **Handling Sparse/Dense Data and Outliers:**\n",
    "   - Assess the density of the data.\n",
    "   - Propose strategies to handle missing data and outliers while maintaining dataset integrity.\n",
    "\n",
    "   **Insert code for Handling Sparse/Dense Data and Outliers below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f_R52s5RE_wj",
   "metadata": {
    "id": "f_R52s5RE_wj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5SS3lD0sGPGe",
   "metadata": {
    "id": "5SS3lD0sGPGe"
   },
   "source": [
    "6. **Data Splitting:**\n",
    "   - Define a methodology to split the dataset into training, validation, and testing sets.\n",
    "   - Ensure randomness and representativeness in each subset.\n",
    "\n",
    "7. **Bias Mitigation:**\n",
    "   - Implement techniques to identify and mitigate biases in the dataset.\n",
    "   - Ensure fairness and equity in data representation.\n",
    "   \n",
    "    **Your answer for Hypothesis Testing goes here **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iX_5Hl65GEgY",
   "metadata": {
    "id": "iX_5Hl65GEgY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "OzGrkCCXGUxV",
   "metadata": {
    "id": "OzGrkCCXGUxV"
   },
   "source": [
    "8. **Features for Model Training:**\n",
    "   - Identify relevant features for training the model.\n",
    "   - Rank features based on their significance to project objectives.\n",
    "\n",
    " **Your answer for features must be plotted/ show your working code-wise **\n",
    "\n",
    "9. **Types of Data Handling:**\n",
    "   - Classify the types of data (categorical, numerical, etc.) present in the dataset.\n",
    "   - Plan preprocessing steps for each data type.\n",
    "\n",
    "   [**insert text for preprocessing steps**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YNkqADAQGbNZ",
   "metadata": {
    "id": "YNkqADAQGbNZ"
   },
   "outputs": [],
   "source": [
    "#print out relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lG_mx92GGX6W",
   "metadata": {
    "id": "lG_mx92GGX6W"
   },
   "source": [
    "\n",
    "10. **Data Transformation for Modeling:**\n",
    "    - Specify methods for transforming raw data into a model-friendly format.\n",
    "    - Detail steps for normalization, scaling, or encoding categorical variables.\n",
    "\n",
    "11. **Data Storage:**\n",
    "    - Determine where and how processed data will be stored.\n",
    "    - Choose suitable storage solutions ensuring accessibility and security.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes:\n",
    "- This template provides a structured framework for documenting your data processing approach for the portfolio project.\n",
    "- Fill out each section with specific details relevant to your project's requirements and objectives.\n",
    "- Use additional cells as needed to provide comprehensive information."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "runpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
